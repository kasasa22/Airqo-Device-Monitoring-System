name: AirQo Device Monitoring CI/CD Pipeline

on:
  push:
    branches: [ main, temp_branch ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual triggering

jobs:
  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort pytest
          pip install -r requirements.txt
          if [ -f backend/api/requirements.txt ]; then pip install -r backend/api/requirements.txt; fi

      - name: Lint with flake8
        run: |
          # Stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # Exit-zero treats all errors as warnings
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Check formatting with black
        run: |
          black --check --diff backend dags tests

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install Node.js dependencies
        run: |
          cd frontend
          npm ci

      - name: Lint JavaScript/React
        run: |
          cd frontend
          npm run lint || echo "Linting completed with warnings"

  test-backend:
    name: Test Backend API
    needs: lint
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          pip install -r requirements.txt
          pip install -r backend/api/requirements.txt

      - name: Set up test directory
        run: |
          mkdir -p tests/unit/api
          mkdir -p tests/integration/api_db
          touch tests/unit/api/__init__.py
          touch tests/integration/api_db/__init__.py

      - name: Create test configuration
        run: |
          echo "DATABASE_URL=postgresql://airflow:airflow@localhost:5432/airflow_test" > .env.test
          echo "AIRQO_API_TOKEN=${{ secrets.AIRQO_API_TOKEN || 'test_token' }}" >> .env.test

      - name: Run unit tests
        env:
          DATABASE_URL: postgresql://airflow:airflow@localhost:5432/airflow_test
          AIRQO_API_TOKEN: ${{ secrets.AIRQO_API_TOKEN || 'test_token' }}
        run: |
          pytest tests/unit/api --cov=backend/api -v

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://airflow:airflow@localhost:5432/airflow_test
          AIRQO_API_TOKEN: ${{ secrets.AIRQO_API_TOKEN || 'test_token' }}
        run: |
          pytest tests/integration/api_db -v

      - name: Upload coverage report
        uses: codecov/codecov-action@v3
        with:
          fail_ci_if_error: false

  test-airflow-dags:
    name: Test Airflow DAGs
    needs: lint
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install Airflow and dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "apache-airflow==2.5.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.5.1/constraints-3.10.txt"
          pip install pytest pytest-cov pandas
          if [ -f dags/requirements.txt ]; then pip install -r dags/requirements.txt; fi

      - name: Set up Airflow home and configuration
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow_home
          mkdir -p $AIRFLOW_HOME/dags
          mkdir -p $AIRFLOW_HOME/plugins
          mkdir -p $AIRFLOW_HOME/logs
          mkdir -p tests/unit/dags
          touch tests/unit/dags/__init__.py
          cp -r dags/* $AIRFLOW_HOME/dags/
          cp -r plugins/* $AIRFLOW_HOME/plugins/
          echo "AIRFLOW__CORE__DAGS_FOLDER=$AIRFLOW_HOME/dags" > $AIRFLOW_HOME/airflow.cfg
          echo "AIRFLOW__CORE__PLUGINS_FOLDER=$AIRFLOW_HOME/plugins" >> $AIRFLOW_HOME/airflow.cfg
          echo "AIRFLOW__CORE__LOAD_EXAMPLES=False" >> $AIRFLOW_HOME/airflow.cfg
          echo "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@localhost:5432/airflow_test" >> $AIRFLOW_HOME/airflow.cfg

      - name: Set environment variables
        run: |
          echo "AIRFLOW_HOME=$(pwd)/airflow_home" >> $GITHUB_ENV
          echo "AIRQO_API_TOKEN=${{ secrets.AIRQO_API_TOKEN || 'test_token' }}" >> $GITHUB_ENV

      - name: Test DAG validation
        run: |
          for dag_file in $AIRFLOW_HOME/dags/*.py; do
            echo "Validating dag file: $dag_file"
            python -c "import sys; from airflow.utils.dag_cycle_tester import check_cycle; from airflow.models.dag import DAG; sys.path.append('$AIRFLOW_HOME'); __import__('$(basename ${dag_file%.py})')"
          done

      - name: Run DAG unit tests
        run: |
          pytest tests/unit/dags -v

  test-dbt:
    name: Test DBT Models
    needs: lint
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install DBT and dependencies
        run: |
          python -m pip install --upgrade pip
          pip install dbt-postgres==1.5.0

      - name: Set up DBT profile
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << EOF
          airqo:
            target: test
            outputs:
              test:
                type: postgres
                host: localhost
                user: airflow
                password: airflow
                port: 5432
                dbname: airflow_test
                schema: dbt_test
                threads: 1
          EOF

      - name: Run DBT tests
        if: hashFiles('dbt/dbt_project.yml') != ''
        run: |
          cd dbt
          dbt debug
          dbt test --target test || echo "DBT tests completed with warnings"

  test-frontend:
    name: Test Frontend
    needs: lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: |
          cd frontend
          npm ci

      - name: Run unit tests
        run: |
          cd frontend
          npm test -- --watchAll=false

      - name: Create build for testing
        run: |
          cd frontend
          npm run build

  docker-build-test:
    name: Test Docker Builds
    needs: [test-backend, test-airflow-dags, test-frontend]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build FastAPI image
        uses: docker/build-push-action@v4
        with:
          context: ./backend/api
          push: false
          load: true
          tags: airqo-api:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build Airflow image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./dockerfile
          push: false
          load: true
          tags: airqo-airflow:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Test containers
        run: |
          docker network create test-network
          docker run -d --name postgres-test --network test-network -e POSTGRES_USER=airflow -e POSTGRES_PASSWORD=airflow -e POSTGRES_DB=airflow postgres:13
          sleep 10 # Wait for postgres to start
          docker run --network test-network -e DATABASE_URL=postgresql://airflow:airflow@postgres-test:5432/airflow airqo-api:test python -c "from app.database import engine; from sqlalchemy import text; with engine.connect() as conn: conn.execute(text('SELECT 1'))"
          docker run --network test-network -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-test:5432/airflow airqo-airflow:test airflow version

  deploy:
    name: Deploy Application
    needs: [docker-build-test]
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push FastAPI image
        uses: docker/build-push-action@v4
        with:
          context: ./backend/api
          push: true
          tags: ${{ secrets.DOCKERHUB_USERNAME }}/airqo-api:latest,${{ secrets.DOCKERHUB_USERNAME }}/airqo-api:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Airflow image
        uses: docker/build-push-action@v4
        with:
          context: .
          file: ./dockerfile
          push: true
          tags: ${{ secrets.DOCKERHUB_USERNAME }}/airqo-airflow:latest,${{ secrets.DOCKERHUB_USERNAME }}/airqo-airflow:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Deploy to server
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.DEPLOY_HOST }}
          username: ${{ secrets.DEPLOY_USERNAME }}
          key: ${{ secrets.DEPLOY_KEY }}
          script: |
            cd ~/Airqo-Device-Monitoring-System
            git pull
            export DOCKERHUB_USERNAME=${{ secrets.DOCKERHUB_USERNAME }}
            docker-compose -f docker-compose.prod.yml pull
            docker-compose -f docker-compose.prod.yml up -d
            docker system prune -f